{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalising the data, and turning it into a tensor (obviously)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# offsets the mean by half, the divides by std. dev.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "classes = list(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5,), std=(0.5,))\n",
       "           )"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    4     2     7     4\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "# show images \n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(images.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=4056, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(6*26*26, 120) # no padding, image will shrink on edges after conv \n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         print(x.size())\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "#         print(x.size())\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "0, 199, 0.7651179490238428\n",
      "0, 399, 0.26384268414229156\n",
      "0, 599, 0.17389392137527465\n",
      "0, 799, 0.1381122137606144\n",
      "epoch 1\n",
      "1, 199, 0.09153694389387965\n",
      "1, 399, 0.08596958797425032\n",
      "1, 599, 0.07576394645497203\n",
      "1, 799, 0.07600419702008367\n",
      "epoch 2\n",
      "2, 199, 0.05106888005509973\n",
      "2, 399, 0.058225299529731274\n",
      "2, 599, 0.04764415230602026\n",
      "2, 799, 0.05307802174240351\n",
      "epoch 3\n",
      "3, 199, 0.031511138156056405\n",
      "3, 399, 0.030702428855001927\n",
      "3, 599, 0.03753732534125447\n",
      "3, 799, 0.03789238030090928\n",
      "epoch 4\n",
      "4, 199, 0.020477833971381188\n",
      "4, 399, 0.025647509619593622\n",
      "4, 599, 0.022448669634759427\n",
      "4, 799, 0.023817011564970018\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "N_epochs = 5\n",
    "print_every = 200\n",
    "for epoch in range(N_epochs):\n",
    "    print(f'epoch {epoch}')\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = [d.to('cuda') for d in data]\n",
    "        optimizer.zero_grad() # reset gradient buffers\n",
    "        output = net(inputs) # run net on input, get output\n",
    "        loss = criterion(output, labels) # compute loss\n",
    "        loss.backward() # compute gradient (dweights/dloss)\n",
    "        optimizer.step() # update weights (using SGD for example)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % print_every == print_every - 1:\n",
    "            print(f'{epoch}, {i}, {running_loss / print_every}')\n",
    "            running_loss = 0.0\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5701, -0.3253, -0.6578],\n",
       "        [-0.1640, -0.5507, -0.1456],\n",
       "        [-0.5413, -0.1629, -0.9822]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3,3).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './mnist_net_1.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: [4, 4, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(f'labels: {list(classes[labels[j]] for j in range(4))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.5128, -1.8805,  0.6608, -4.3693, 13.5946, -2.3398, -1.7257,  0.6736,\n",
      "          0.5256,  1.0267],\n",
      "        [-3.8483, -0.4576, -3.7708, -7.8454, 18.3886, -1.7589, -1.3990,  2.3793,\n",
      "         -0.3322,  1.7518],\n",
      "        [ 9.7792, -2.9378,  1.7730, -4.6323, -1.3177, -2.1759,  0.1541, -2.5081,\n",
      "          1.4952,  0.5971],\n",
      "        [-5.9833, 13.1466, -2.1820, -1.9522,  2.0484,  0.7841, -1.6817, -0.7419,\n",
      "          0.3162, -2.5264]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "outputs = net(images)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.594603538513184: 4\n",
      "18.388648986816406: 4\n",
      "9.779165267944336: 0\n",
      "13.146590232849121: 1\n",
      "tensor([4, 4, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "for o in outputs:\n",
    "    l = [(v, classes[i]) for i,v in enumerate(o)]\n",
    "    l.sort(reverse=True)\n",
    "    print(f'{l[0][0]}: {l[0][1]}')\n",
    "print(labels)\n",
    "# AYYYYY, now do the whole test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9848\n"
     ]
    }
   ],
   "source": [
    "correct, total = 0, 0\n",
    "with torch.no_grad(): #REMEMBER TO TURN OFF AUTOGRAD\n",
    "    for data in testloader:\n",
    "        images, labels = [d.to('cuda') for d in data]\n",
    "        outputs = net(images)\n",
    "        maxval, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'acc: {correct/total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 26, 26]              60\n",
      "            Linear-2                  [-1, 120]         486,840\n",
      "            Linear-3                   [-1, 10]           1,210\n",
      "================================================================\n",
      "Total params: 488,110\n",
      "Trainable params: 488,110\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.03\n",
      "Params size (MB): 1.86\n",
      "Estimated Total Size (MB): 1.90\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(net, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
